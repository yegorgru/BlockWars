behaviors:
  MapController:
    trainer_type:   ppo
    hyperparameters:
      batch_size: 128 #opt_values: [8, 32, 128, 512]
      buffer_size: 8192 #opt_values: [2048, 8192, 32768, 65536]
      learning_rate:  0.0003  #opt_values: [0.003, 0.0003, 0.00003]
      beta: 0.01  #opt_values: [0.0001, 0.001, 0.005, 0.01]
      epsilon:      0.2  #opt_values: [0.1, 0.2, 0.3]
      lambd:        0.925  #opt_values: [0.9, 0.925, 0.95]
      num_epoch:    2    #opt_values: [2, 3, 6, 9]
      shared_critic:        False
      learning_rate_schedule:       linear
      beta_schedule:        linear
      epsilon_schedule:     linear
    network_settings:
      normalize:    False
      hidden_units: 32   #opt_values: [32, 128, 256, 512]
      num_layers:   3 #opt_values: [2, 3, 4]
      vis_encode_type:      simple
      goal_conditioning_type:       hyper
      deterministic:        False
    reward_signals:
      extrinsic:
        gamma:      0.9  #opt_values: [0.8, 0.9, 0.99, 0.995]
        strength:   1.0
        network_settings:
          normalize:        False
          hidden_units:     32
          num_layers:       3
          vis_encode_type:  simple
          goal_conditioning_type:   hyper
          deterministic:    False
    keep_checkpoints:       5
    checkpoint_interval:    50000
    max_steps:      300000
    time_horizon:   64  #opt_values: [64, 512, 1024, 2048]
    summary_freq:   25000
    threaded:       False